{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-hp/LSTM/blob/master/intento_LSTM\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-iuPcYuLDA",
        "colab_type": "text"
      },
      "source": [
        "## Se importan las bibliotecas necesarias de tensorflow y keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHkBsotVoZUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, ConvLSTM2D, Flatten\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import Input\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GgQ3zM5u4ry",
        "colab_type": "text"
      },
      "source": [
        "##Se crea una función que normaliza a un elemento de un conjunto, dado el elemento máximo y mínimo de dicho conjunto, posteriormente multiplica al elemento normalizado por 10, para tener valores en el rango [0,10]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKjAh13qugvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normaliza(mayor, minimo, elem):\n",
        "    if mayor == 0:\n",
        "        return 0\n",
        "    return ((elem - minimo) / (mayor - minimo)) * 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf0I46glvIIp",
        "colab_type": "text"
      },
      "source": [
        "##Se crea una función que procesa la entrada y normaliza los valores de las columnas de acorde al elemento máximo y mínimo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y2bhOvLumLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def procesaEntrada(arr, h1_m=101, w1_m=199, h2_m=199, w2_m=199):\n",
        "    if len(arr) > 0:\n",
        "        h1 = arr[:, 2]\n",
        "        w1 = arr[:, 3]\n",
        "        h2 = arr[:, 4]\n",
        "        w2 = arr[:, 5]\n",
        "\n",
        "        nv_h1 = copy.deepcopy(h1)\n",
        "        nv_w1 = copy.deepcopy(w1)\n",
        "        nv_h2 = copy.deepcopy(h2)\n",
        "        nv_w2 = copy.deepcopy(w2)\n",
        "\n",
        "        idx = 0\n",
        "        for a, b, c, d in zip(w1, h1, w2, h2):\n",
        "            nv_h1[idx] = normaliza(h1_m, 0, b)\n",
        "            nv_w1[idx] = normaliza(w1_m, 0, a)\n",
        "            nv_h2[idx] = normaliza(h2_m, 0, d)\n",
        "            nv_w2[idx] = normaliza(w2_m, 0, c)\n",
        "            idx += 1\n",
        "        return np.column_stack((np.array(arr[:, 0]), np.array(arr[:, 1]), np.array(nv_h1), np.array(nv_w1),\n",
        "                                np.array(nv_h2), np.array(nv_w2)))\n",
        "    else:\n",
        "        return []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vX4Mz2vQQt",
        "colab_type": "text"
      },
      "source": [
        "Se procesa la entrada y se entrena al modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DuWjbAMuAhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85b0bdbb-2d1a-470d-8568-a45b98da5f78"
      },
      "source": [
        "# Se lee el conjunto de señales\n",
        "pckl = pd.read_pickle(\"conjunto_total.pckl\")\n",
        "\n",
        "# Se \n",
        "tag_to_indx = {\"Agua\": 1, \"Alto\": 2, \"Apagar\": 3, \"Ayudar\": 4, \"Campo\": 5, \"Crecer\": 6, \"Cuerpo\": 7, \"Despedir\": 8,\n",
        "               \"Dinero\": 9, \"Escribir\": 10, \"Foto\": 11, \"Fruto\": 12, \"Hacer\": 13, \"LavarCara\": 14, \"Madera\": 15,\n",
        "               \"Miedo\": 16, \"Morder\": 17, \"Nariz\": 18, \"Padrino\": 19, \"Película\": 20, \"Persona\": 21, \"Primero\": 22,\n",
        "               \"Quedar\": 23, \"Saber\": 24, \"Sangre\": 25, \"Señorita\": 26, \"Suerte\": 27, \"Toro\": 28}\n",
        "indx_to_tag = {1: \"Agua\", 2: \"Alto\", 3: \"Apagar\", 4: \"Ayudar\", 5: \"Campo\", 6: \"Crecer\", 7: \"Cuerpo\", 8: \"Despedir\",\n",
        "               9: \"Dinero\", 10: \"Escribir\", 11: \"Foto\", 12: \"Fruto\", 13: \"Hacer\", 14: \"LavarCara\", 15: \"Madera\",\n",
        "               16: \"Miedo\", 17: \"Morder\", 18: \"Nariz\", 19: \"Padrino\", 20: \"Película\", 21: \"Persona\", 22: \"Primero\",\n",
        "               23: \"Quedar\", 24: \"Saber\", 25: \"Sangre\", 26: \"Señorita\", 27: \"Suerte\", 28: \"Toro\"}\n",
        "X = pckl.iloc[:, 0].values\n",
        "\n",
        "max_h1, max_w1, max_h2, max_w2 = 0, 0, 0, 0\n",
        "# Se obtienen los valores máximos de las columnas del conjunto de señales, esto para normalizar\n",
        "for p in X:\n",
        "    for m in p:\n",
        "        if m[2] > max_h1:\n",
        "            max_h1 = m[2]\n",
        "        if m[3] > max_w1:\n",
        "            max_w1 = m[3]\n",
        "        if m[4] > max_h2:\n",
        "            max_h2 = m[4]\n",
        "        if m[5] > max_w2:\n",
        "            max_w2 = m[5]\n",
        "X = [procesaEntrada(l, max_h1, max_w1, max_h2, max_w2) for l in X]\n",
        "\n",
        "\n",
        "y = pckl.iloc[:, 1].values\n",
        "\n",
        "y = [tag_to_indx[x] for x in y]\n",
        "\n",
        "MAX_LENGTH = len(max(X, key=len))\n",
        "# Se hace todas las secuencias tengan el mismo número de pasos, esto concatenando 0 al final de cada secuencia con tamaño inferior\n",
        "arreglo_total = pad_sequences(maxlen=MAX_LENGTH, sequences=X, padding=\"post\")\n",
        "\n",
        "print(arreglo_total)\n",
        "\n",
        "# Se divide al conjunto en 80% para entrenamiento y 20% para prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(arreglo_total, y, test_size=0.2)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "# Se definen las epoch y el tamaño de los lotes\n",
        "verbose, epochs, batch_size = 1, 1000, 128\n",
        "# Se definen el número de pasos, de características y de entradas\n",
        "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 1\n",
        "# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "n_steps, n_length = n_timesteps, 1\n",
        "\n",
        "# Se modifican las dimensiones de las secuencias para poder procesarlas\n",
        "trainX = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
        "testX = X_test.reshape((X_test.shape[0], n_steps, 1, n_length, n_features))\n",
        "# Se define el modelo\n",
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(padding='same',filters=1, kernel_size=(1, 3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(n_outputs, activation='softmax'))\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "# Se genera un EarlyStopping, el cual se detiene si no ha habido mejores en la accuracy durante 10 epochs al entrenar\n",
        "chk1 = EarlyStopping(monitor='acc', min_delta=0, patience=10, verbose=1, mode='auto', baseline=None, restore_best_weights=False)\n",
        "callbacks_list = [chk1]\n",
        "\n",
        "# Se entrena al modelo\n",
        "model.fit(trainX, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,callbacks=callbacks_list)\n",
        "# Se evalúa la accuracy del modelo\n",
        "_, accuracy = model.evaluate(testX, y_test, batch_size=batch_size, verbose=0)\n",
        "print(accuracy)\n",
        "model.save('my_model.h5')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[18 18  0  0  0  0]\n",
            "  [ 2 18  2  2  0  0]\n",
            "  [18 18  0  0  0  0]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]\n",
            "\n",
            " [[ 1 18  5  3  0  0]\n",
            "  [18 18  4  3  0  0]\n",
            "  [18 18  4  3  0  0]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]\n",
            "\n",
            " [[ 2 18  2  2  0  0]\n",
            "  [18 18  3  2  0  0]\n",
            "  [18 18  2  2  0  0]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1  2  4  2  4  3]\n",
            "  [18 14  4  3  3  2]\n",
            "  [ 4  3  5  4  4  3]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]\n",
            "\n",
            " [[18 18  0  0  0  0]\n",
            "  [18 18  0  0  0  0]\n",
            "  [18 18  0  0  0  0]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]\n",
            "\n",
            " [[ 2 18  2  1  0  0]\n",
            "  [18  3  2  2  2  1]\n",
            "  [18 18  2  1  2  1]\n",
            "  ...\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0816 04:52:28.311031 140106187982720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "900/900 [==============================] - 18s 20ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 2/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 3/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 4/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 5/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 6/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 7/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 8/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 9/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 10/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 11/1000\n",
            "900/900 [==============================] - 17s 19ms/step - loss: 246.1256 - acc: 0.0333\n",
            "Epoch 00011: early stopping\n",
            "0.035555555009179646\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}