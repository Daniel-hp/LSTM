{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-hp/LSTM/blob/master/Resultados%20del%20entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99pN0N4YNXj",
        "colab_type": "text"
      },
      "source": [
        "### **Entrenamiento de la red sobre los nuevos videos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI4pm-DjW4lW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "986c7c44-c8c1-4659-b659-e24a46b61768"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1996)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1996)\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import  TimeseriesGenerator\n",
        "from keras.layers import LSTM, Conv1D, MaxPool1D, Flatten, CuDNNGRU, CuDNNLSTM, Flatten\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import keras as keras\n",
        "from keras.optimizers import  rmsprop\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = np.load('/content/gdrive/My Drive/openpose_lstm/trainX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/y_train.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/testX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/y_test.npy')\n",
        "\n",
        "tam_input = 30\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(CuDNNLSTM(10, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Conv1D(filters=15, kernel_size=15 ))\n",
        "model.add(MaxPool1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(28, activation='softmax'))\n",
        "opt = rmsprop(lr=0.001, rho=0.9)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "print(model.summary())\n",
        "batch_size = 256\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('model_lstm2{epoch:08d}.h5', period=350)\n",
        "history = model.fit(x=X_train, y= Y_train, epochs=150, batch_size=batch_size, validation_data=(X_test, Y_test) )\n",
        "\n",
        "model.save('/content/gdrive/My Drive/openpose_lstm/modelo_def.h5')\n",
        "validation_acc = np.amax(history.history['val_acc'])\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/acc2.png')\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/loss2.png')\n",
        "\n",
        "print('Best validation acc of epoch:', validation_acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 2604, 10)          560       \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_4 (CuDNNLSTM)     (None, 2604, 10)          880       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 2590, 15)          2265      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 1295, 15)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 19425)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 28)                543928    \n",
            "=================================================================\n",
            "Total params: 547,633\n",
            "Trainable params: 547,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 9040 samples, validate on 1130 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "9040/9040 [==============================] - 23s 3ms/step - loss: 2.7402 - acc: 0.2423 - val_loss: 2.4243 - val_acc: 0.2982\n",
            "Epoch 2/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 1.7709 - acc: 0.4531 - val_loss: 1.8738 - val_acc: 0.3956\n",
            "Epoch 3/150\n",
            "9040/9040 [==============================] - 8s 939us/step - loss: 1.3423 - acc: 0.5713 - val_loss: 1.4591 - val_acc: 0.5248\n",
            "Epoch 4/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 1.0501 - acc: 0.6538 - val_loss: 1.1134 - val_acc: 0.6283\n",
            "Epoch 5/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.8919 - acc: 0.7048 - val_loss: 0.9023 - val_acc: 0.7044\n",
            "Epoch 6/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.7559 - acc: 0.7540 - val_loss: 0.9471 - val_acc: 0.7080\n",
            "Epoch 7/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.6805 - acc: 0.7778 - val_loss: 0.8583 - val_acc: 0.7115\n",
            "Epoch 8/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.5918 - acc: 0.8127 - val_loss: 1.1035 - val_acc: 0.7000\n",
            "Epoch 9/150\n",
            "9040/9040 [==============================] - 9s 941us/step - loss: 0.5427 - acc: 0.8272 - val_loss: 0.7467 - val_acc: 0.7681\n",
            "Epoch 10/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.4939 - acc: 0.8376 - val_loss: 0.7796 - val_acc: 0.7611\n",
            "Epoch 11/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.4498 - acc: 0.8540 - val_loss: 0.6327 - val_acc: 0.8115\n",
            "Epoch 12/150\n",
            "9040/9040 [==============================] - 8s 936us/step - loss: 0.4127 - acc: 0.8689 - val_loss: 0.5868 - val_acc: 0.8265\n",
            "Epoch 13/150\n",
            "9040/9040 [==============================] - 8s 936us/step - loss: 0.3707 - acc: 0.8808 - val_loss: 0.6145 - val_acc: 0.8283\n",
            "Epoch 14/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.3551 - acc: 0.8865 - val_loss: 0.5013 - val_acc: 0.8513\n",
            "Epoch 15/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.3134 - acc: 0.9019 - val_loss: 0.5610 - val_acc: 0.8292\n",
            "Epoch 16/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.2976 - acc: 0.9053 - val_loss: 0.6743 - val_acc: 0.8239\n",
            "Epoch 17/150\n",
            "9040/9040 [==============================] - 8s 937us/step - loss: 0.2948 - acc: 0.9072 - val_loss: 0.4838 - val_acc: 0.8664\n",
            "Epoch 18/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.2504 - acc: 0.9244 - val_loss: 0.4636 - val_acc: 0.8699\n",
            "Epoch 19/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.2482 - acc: 0.9227 - val_loss: 0.6231 - val_acc: 0.8372\n",
            "Epoch 20/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.2289 - acc: 0.9311 - val_loss: 0.5333 - val_acc: 0.8558\n",
            "Epoch 21/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.2246 - acc: 0.9336 - val_loss: 0.5634 - val_acc: 0.8212\n",
            "Epoch 22/150\n",
            "9040/9040 [==============================] - 8s 935us/step - loss: 0.2009 - acc: 0.9399 - val_loss: 0.4800 - val_acc: 0.8708\n",
            "Epoch 23/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.1999 - acc: 0.9386 - val_loss: 0.4016 - val_acc: 0.9035\n",
            "Epoch 24/150\n",
            "9040/9040 [==============================] - 8s 936us/step - loss: 0.1955 - acc: 0.9402 - val_loss: 0.4357 - val_acc: 0.8885\n",
            "Epoch 25/150\n",
            "9040/9040 [==============================] - 9s 941us/step - loss: 0.1547 - acc: 0.9559 - val_loss: 0.5558 - val_acc: 0.8637\n",
            "Epoch 26/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.1757 - acc: 0.9458 - val_loss: 0.4963 - val_acc: 0.8788\n",
            "Epoch 27/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.1632 - acc: 0.9515 - val_loss: 0.4024 - val_acc: 0.9133\n",
            "Epoch 28/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.1378 - acc: 0.9579 - val_loss: 0.4463 - val_acc: 0.8832\n",
            "Epoch 29/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.1405 - acc: 0.9605 - val_loss: 0.4339 - val_acc: 0.8885\n",
            "Epoch 30/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.1280 - acc: 0.9632 - val_loss: 0.3363 - val_acc: 0.9186\n",
            "Epoch 31/150\n",
            "9040/9040 [==============================] - 8s 937us/step - loss: 0.1280 - acc: 0.9624 - val_loss: 0.4629 - val_acc: 0.8965\n",
            "Epoch 32/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.1189 - acc: 0.9681 - val_loss: 0.4441 - val_acc: 0.8903\n",
            "Epoch 33/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.1126 - acc: 0.9678 - val_loss: 0.5386 - val_acc: 0.8673\n",
            "Epoch 34/150\n",
            "9040/9040 [==============================] - 9s 940us/step - loss: 0.1075 - acc: 0.9694 - val_loss: 0.3322 - val_acc: 0.9292\n",
            "Epoch 35/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.1001 - acc: 0.9698 - val_loss: 0.4450 - val_acc: 0.9000\n",
            "Epoch 36/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0928 - acc: 0.9736 - val_loss: 0.3781 - val_acc: 0.9115\n",
            "Epoch 37/150\n",
            "9040/9040 [==============================] - 9s 940us/step - loss: 0.0858 - acc: 0.9765 - val_loss: 0.6104 - val_acc: 0.8637\n",
            "Epoch 38/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0907 - acc: 0.9749 - val_loss: 0.3222 - val_acc: 0.9327\n",
            "Epoch 39/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.0746 - acc: 0.9805 - val_loss: 0.6576 - val_acc: 0.8496\n",
            "Epoch 40/150\n",
            "9040/9040 [==============================] - 8s 939us/step - loss: 0.0807 - acc: 0.9773 - val_loss: 0.4645 - val_acc: 0.8947\n",
            "Epoch 41/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0721 - acc: 0.9812 - val_loss: 0.4022 - val_acc: 0.9150\n",
            "Epoch 42/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0681 - acc: 0.9819 - val_loss: 0.3571 - val_acc: 0.9292\n",
            "Epoch 43/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0727 - acc: 0.9812 - val_loss: 0.3374 - val_acc: 0.9327\n",
            "Epoch 44/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0714 - acc: 0.9821 - val_loss: 0.3271 - val_acc: 0.9487\n",
            "Epoch 45/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0605 - acc: 0.9834 - val_loss: 0.3598 - val_acc: 0.9372\n",
            "Epoch 46/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0554 - acc: 0.9855 - val_loss: 0.2955 - val_acc: 0.9531\n",
            "Epoch 47/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0596 - acc: 0.9848 - val_loss: 0.3064 - val_acc: 0.9504\n",
            "Epoch 48/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0549 - acc: 0.9853 - val_loss: 0.4329 - val_acc: 0.9336\n",
            "Epoch 49/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0530 - acc: 0.9854 - val_loss: 0.3390 - val_acc: 0.9283\n",
            "Epoch 50/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0672 - acc: 0.9815 - val_loss: 0.3031 - val_acc: 0.9540\n",
            "Epoch 51/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0487 - acc: 0.9861 - val_loss: 0.4189 - val_acc: 0.9062\n",
            "Epoch 52/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0390 - acc: 0.9899 - val_loss: 0.2811 - val_acc: 0.9531\n",
            "Epoch 53/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0460 - acc: 0.9869 - val_loss: 0.3005 - val_acc: 0.9504\n",
            "Epoch 54/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0435 - acc: 0.9877 - val_loss: 0.4914 - val_acc: 0.9071\n",
            "Epoch 55/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.3643 - val_acc: 0.9354\n",
            "Epoch 56/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0378 - acc: 0.9895 - val_loss: 0.3803 - val_acc: 0.9389\n",
            "Epoch 57/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.0431 - acc: 0.9888 - val_loss: 0.2710 - val_acc: 0.9593\n",
            "Epoch 58/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0340 - acc: 0.9927 - val_loss: 0.3718 - val_acc: 0.9416\n",
            "Epoch 59/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0425 - acc: 0.9876 - val_loss: 0.3783 - val_acc: 0.9265\n",
            "Epoch 60/150\n",
            "9040/9040 [==============================] - 9s 953us/step - loss: 0.0316 - acc: 0.9914 - val_loss: 0.2803 - val_acc: 0.9602\n",
            "Epoch 61/150\n",
            "9040/9040 [==============================] - 9s 940us/step - loss: 0.0323 - acc: 0.9912 - val_loss: 0.2853 - val_acc: 0.9593\n",
            "Epoch 62/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0418 - acc: 0.9894 - val_loss: 0.3774 - val_acc: 0.9434\n",
            "Epoch 63/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0263 - acc: 0.9939 - val_loss: 0.2834 - val_acc: 0.9619\n",
            "Epoch 64/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0352 - acc: 0.9920 - val_loss: 0.2749 - val_acc: 0.9628\n",
            "Epoch 65/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0294 - acc: 0.9936 - val_loss: 0.3662 - val_acc: 0.9451\n",
            "Epoch 66/150\n",
            "9040/9040 [==============================] - 8s 939us/step - loss: 0.0280 - acc: 0.9929 - val_loss: 0.2859 - val_acc: 0.9646\n",
            "Epoch 67/150\n",
            "9040/9040 [==============================] - 9s 940us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.6697 - val_acc: 0.8761\n",
            "Epoch 68/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.0342 - acc: 0.9917 - val_loss: 0.4314 - val_acc: 0.9398\n",
            "Epoch 69/150\n",
            "9040/9040 [==============================] - 9s 941us/step - loss: 0.0383 - acc: 0.9909 - val_loss: 0.2780 - val_acc: 0.9611\n",
            "Epoch 70/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0270 - acc: 0.9936 - val_loss: 0.3116 - val_acc: 0.9549\n",
            "Epoch 71/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0276 - acc: 0.9937 - val_loss: 0.3076 - val_acc: 0.9558\n",
            "Epoch 72/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0349 - acc: 0.9929 - val_loss: 0.3037 - val_acc: 0.9575\n",
            "Epoch 73/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0196 - acc: 0.9958 - val_loss: 0.3386 - val_acc: 0.9602\n",
            "Epoch 74/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0271 - acc: 0.9937 - val_loss: 0.3569 - val_acc: 0.9442\n",
            "Epoch 75/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0313 - acc: 0.9927 - val_loss: 0.3162 - val_acc: 0.9593\n",
            "Epoch 76/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0202 - acc: 0.9969 - val_loss: 0.3171 - val_acc: 0.9602\n",
            "Epoch 77/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0225 - acc: 0.9948 - val_loss: 0.3897 - val_acc: 0.9407\n",
            "Epoch 78/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0308 - acc: 0.9933 - val_loss: 0.3415 - val_acc: 0.9522\n",
            "Epoch 79/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0225 - acc: 0.9950 - val_loss: 0.3071 - val_acc: 0.9637\n",
            "Epoch 80/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0276 - acc: 0.9946 - val_loss: 0.3183 - val_acc: 0.9628\n",
            "Epoch 81/150\n",
            "9040/9040 [==============================] - 8s 937us/step - loss: 0.0271 - acc: 0.9945 - val_loss: 0.2948 - val_acc: 0.9619\n",
            "Epoch 82/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0176 - acc: 0.9969 - val_loss: 0.2961 - val_acc: 0.9619\n",
            "Epoch 83/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0305 - acc: 0.9925 - val_loss: 0.3459 - val_acc: 0.9522\n",
            "Epoch 84/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0199 - acc: 0.9961 - val_loss: 0.4148 - val_acc: 0.9398\n",
            "Epoch 85/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0176 - acc: 0.9971 - val_loss: 0.3110 - val_acc: 0.9602\n",
            "Epoch 86/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0229 - acc: 0.9954 - val_loss: 0.3024 - val_acc: 0.9619\n",
            "Epoch 87/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0238 - acc: 0.9945 - val_loss: 0.3003 - val_acc: 0.9602\n",
            "Epoch 88/150\n",
            "9040/9040 [==============================] - 9s 951us/step - loss: 0.0206 - acc: 0.9954 - val_loss: 0.3046 - val_acc: 0.9646\n",
            "Epoch 89/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0274 - acc: 0.9951 - val_loss: 0.3222 - val_acc: 0.9575\n",
            "Epoch 90/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.0207 - acc: 0.9960 - val_loss: 0.3245 - val_acc: 0.9602\n",
            "Epoch 91/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0231 - acc: 0.9956 - val_loss: 0.3331 - val_acc: 0.9611\n",
            "Epoch 92/150\n",
            "9040/9040 [==============================] - 9s 941us/step - loss: 0.0151 - acc: 0.9976 - val_loss: 0.3573 - val_acc: 0.9549\n",
            "Epoch 93/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0234 - acc: 0.9959 - val_loss: 0.3132 - val_acc: 0.9637\n",
            "Epoch 94/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0101 - acc: 0.9993 - val_loss: 0.6509 - val_acc: 0.9106\n",
            "Epoch 95/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0195 - acc: 0.9961 - val_loss: 0.2922 - val_acc: 0.9628\n",
            "Epoch 96/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0264 - acc: 0.9949 - val_loss: 0.4028 - val_acc: 0.9354\n",
            "Epoch 97/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0161 - acc: 0.9976 - val_loss: 0.3345 - val_acc: 0.9628\n",
            "Epoch 98/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0230 - acc: 0.9944 - val_loss: 0.3273 - val_acc: 0.9655\n",
            "Epoch 99/150\n",
            "9040/9040 [==============================] - 9s 951us/step - loss: 0.0232 - acc: 0.9950 - val_loss: 0.3092 - val_acc: 0.9602\n",
            "Epoch 100/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0173 - acc: 0.9971 - val_loss: 0.3701 - val_acc: 0.9584\n",
            "Epoch 101/150\n",
            "9040/9040 [==============================] - 9s 952us/step - loss: 0.0199 - acc: 0.9961 - val_loss: 0.4148 - val_acc: 0.9522\n",
            "Epoch 102/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.0235 - acc: 0.9954 - val_loss: 0.3662 - val_acc: 0.9566\n",
            "Epoch 103/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0139 - acc: 0.9979 - val_loss: 0.3275 - val_acc: 0.9628\n",
            "Epoch 104/150\n",
            "9040/9040 [==============================] - 9s 952us/step - loss: 0.0243 - acc: 0.9960 - val_loss: 0.3394 - val_acc: 0.9593\n",
            "Epoch 105/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0178 - acc: 0.9971 - val_loss: 0.3190 - val_acc: 0.9655\n",
            "Epoch 106/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0162 - acc: 0.9978 - val_loss: 0.3264 - val_acc: 0.9664\n",
            "Epoch 107/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0171 - acc: 0.9968 - val_loss: 0.3135 - val_acc: 0.9655\n",
            "Epoch 108/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.0258 - acc: 0.9958 - val_loss: 0.3881 - val_acc: 0.9478\n",
            "Epoch 109/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0113 - acc: 0.9986 - val_loss: 0.3256 - val_acc: 0.9628\n",
            "Epoch 110/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0194 - acc: 0.9971 - val_loss: 0.3246 - val_acc: 0.9646\n",
            "Epoch 111/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0166 - acc: 0.9970 - val_loss: 0.3552 - val_acc: 0.9584\n",
            "Epoch 112/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0125 - acc: 0.9983 - val_loss: 0.6943 - val_acc: 0.9071\n",
            "Epoch 113/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0151 - acc: 0.9972 - val_loss: 0.3244 - val_acc: 0.9611\n",
            "Epoch 114/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0180 - acc: 0.9970 - val_loss: 0.3323 - val_acc: 0.9593\n",
            "Epoch 115/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0165 - acc: 0.9977 - val_loss: 0.3908 - val_acc: 0.9487\n",
            "Epoch 116/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0143 - acc: 0.9979 - val_loss: 0.3517 - val_acc: 0.9611\n",
            "Epoch 117/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0167 - acc: 0.9969 - val_loss: 0.3337 - val_acc: 0.9611\n",
            "Epoch 118/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0252 - acc: 0.9952 - val_loss: 0.3351 - val_acc: 0.9575\n",
            "Epoch 119/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.0104 - acc: 0.9992 - val_loss: 0.3320 - val_acc: 0.9611\n",
            "Epoch 120/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0233 - acc: 0.9966 - val_loss: 0.3355 - val_acc: 0.9602\n",
            "Epoch 121/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0175 - acc: 0.9966 - val_loss: 0.3376 - val_acc: 0.9584\n",
            "Epoch 122/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0210 - acc: 0.9968 - val_loss: 0.3387 - val_acc: 0.9593\n",
            "Epoch 123/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0134 - acc: 0.9981 - val_loss: 0.3403 - val_acc: 0.9593\n",
            "Epoch 124/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0095 - acc: 0.9994 - val_loss: 0.3145 - val_acc: 0.9646\n",
            "Epoch 125/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0198 - acc: 0.9960 - val_loss: 0.3883 - val_acc: 0.9593\n",
            "Epoch 126/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0134 - acc: 0.9983 - val_loss: 0.4973 - val_acc: 0.9363\n",
            "Epoch 127/150\n",
            "9040/9040 [==============================] - 9s 953us/step - loss: 0.0118 - acc: 0.9985 - val_loss: 0.3575 - val_acc: 0.9584\n",
            "Epoch 128/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0191 - acc: 0.9966 - val_loss: 0.3170 - val_acc: 0.9637\n",
            "Epoch 129/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0090 - acc: 0.9994 - val_loss: 0.3192 - val_acc: 0.9628\n",
            "Epoch 130/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0258 - acc: 0.9967 - val_loss: 0.3203 - val_acc: 0.9664\n",
            "Epoch 131/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.0090 - acc: 0.9994 - val_loss: 0.3177 - val_acc: 0.9673\n",
            "Epoch 132/150\n",
            "9040/9040 [==============================] - 9s 942us/step - loss: 0.0226 - acc: 0.9957 - val_loss: 0.3134 - val_acc: 0.9664\n",
            "Epoch 133/150\n",
            "9040/9040 [==============================] - 8s 938us/step - loss: 0.0180 - acc: 0.9970 - val_loss: 0.3379 - val_acc: 0.9611\n",
            "Epoch 134/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0152 - acc: 0.9977 - val_loss: 0.3921 - val_acc: 0.9549\n",
            "Epoch 135/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0194 - acc: 0.9958 - val_loss: 0.3595 - val_acc: 0.9593\n",
            "Epoch 136/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0094 - acc: 0.9994 - val_loss: 0.3227 - val_acc: 0.9619\n",
            "Epoch 137/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0215 - acc: 0.9967 - val_loss: 0.3518 - val_acc: 0.9566\n",
            "Epoch 138/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0096 - acc: 0.9993 - val_loss: 0.3266 - val_acc: 0.9637\n",
            "Epoch 139/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0198 - acc: 0.9968 - val_loss: 0.3570 - val_acc: 0.9637\n",
            "Epoch 140/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0095 - acc: 0.9992 - val_loss: 0.3382 - val_acc: 0.9637\n",
            "Epoch 141/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0231 - acc: 0.9960 - val_loss: 0.3532 - val_acc: 0.9593\n",
            "Epoch 142/150\n",
            "9040/9040 [==============================] - 8s 940us/step - loss: 0.0121 - acc: 0.9981 - val_loss: 0.3420 - val_acc: 0.9628\n",
            "Epoch 143/150\n",
            "9040/9040 [==============================] - 9s 948us/step - loss: 0.0132 - acc: 0.9979 - val_loss: 0.3398 - val_acc: 0.9628\n",
            "Epoch 144/150\n",
            "9040/9040 [==============================] - 9s 945us/step - loss: 0.0188 - acc: 0.9972 - val_loss: 0.5569 - val_acc: 0.9301\n",
            "Epoch 145/150\n",
            "9040/9040 [==============================] - 9s 944us/step - loss: 0.0130 - acc: 0.9983 - val_loss: 0.3670 - val_acc: 0.9566\n",
            "Epoch 146/150\n",
            "9040/9040 [==============================] - 9s 947us/step - loss: 0.0099 - acc: 0.9991 - val_loss: 0.6917 - val_acc: 0.9195\n",
            "Epoch 147/150\n",
            "9040/9040 [==============================] - 9s 949us/step - loss: 0.0183 - acc: 0.9973 - val_loss: 0.3408 - val_acc: 0.9637\n",
            "Epoch 148/150\n",
            "9040/9040 [==============================] - 9s 950us/step - loss: 0.0212 - acc: 0.9962 - val_loss: 0.3585 - val_acc: 0.9611\n",
            "Epoch 149/150\n",
            "9040/9040 [==============================] - 9s 943us/step - loss: 0.0091 - acc: 0.9994 - val_loss: 0.3431 - val_acc: 0.9619\n",
            "Epoch 150/150\n",
            "9040/9040 [==============================] - 9s 946us/step - loss: 0.0154 - acc: 0.9980 - val_loss: 0.3291 - val_acc: 0.9628\n",
            "Best validation acc of epoch: 0.9672566356912123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQYoFo-qe0PH",
        "colab_type": "text"
      },
      "source": [
        "### Gráficas de pérdida y de precisión del modelo, respectivamente\n",
        "\n",
        "![perdida](https://drive.google.com/uc?id=1-G_JepAV2UH1MvP2NMHQPsQ_eH4TSUB7)\n",
        "![precisión](https://drive.google.com/uc?id=1-Gxl6WIPG2P8KB0Lu3q5wKs_MHqKNFSX)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zobnCPgWXXec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f0ed312-0f13-4f1a-a427-99532a4a2634"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = np.load('/content/gdrive/My Drive/openpose_lstm/devX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/y_dev.npy')\n",
        "\n",
        "# define and fit the final model\n",
        "\n",
        "model = load_model('/content/gdrive/My Drive/openpose_lstm/modelo_def.h5')\n",
        "\n",
        "ynew = model.predict_classes(X)\n",
        "# show the inputs and predicted outputs\n",
        "\n",
        "print(\"Longitud conjunto = \" + str(X.shape))\n",
        "y_pred = []\n",
        "y_true = []\n",
        "class_names =  np.array([\"Agua\", \"Alto\", \"Apagar\", \"Ayudar\", \"Campo\", \"Crecer\", \"Cuerpo\", \"Despedir\", \"Dinero\", \"Escribir\", \"Foto\", \"Fruto\", \"Hacer\", \"LavarCara\", \"Madera\", \"Miedo\", \"Morder\", \"Nariz\", \"Padrino\",\"Película\", \"Persona\", \"Primero\", \"Quedar\", \"Saber\", \"Sangre\", \"Señorita\", \"Suerte\", \"Toro\"])\n",
        "tag_to_indx = {\"Agua\": 1, \"Alto\": 2, \"Apagar\": 3, \"Ayudar\": 4, \"Campo\": 5, \"Crecer\": 6, \"Cuerpo\": 7, \"Despedir\": 8,\n",
        "               \"Dinero\": 9, \"Escribir\": 10, \"Foto\": 11, \"Fruto\": 12, \"Hacer\": 13, \"LavarCara\": 14, \"Madera\": 15,\n",
        "               \"Miedo\": 16, \"Morder\": 17, \"Nariz\": 18, \"Padrino\": 19, \"Película\": 20, \"Persona\": 21, \"Primero\": 22,\n",
        "               \"Quedar\": 23, \"Saber\": 24, \"Sangre\": 25, \"Señorita\": 26, \"Suerte\": 27, \"Toro\": 28}\n",
        "indx_to_tag = {1: \"Agua\", 2: \"Alto\", 3: \"Apagar\", 4: \"Ayudar\", 5: \"Campo\", 6: \"Crecer\", 7: \"Cuerpo\", 8: \"Despedir\",\n",
        "               9: \"Dinero\", 10: \"Escribir\", 11: \"Foto\", 12: \"Fruto\", 13: \"Hacer\", 14: \"LavarCara\", 15: \"Madera\",\n",
        "               16: \"Miedo\", 17: \"Morder\", 18: \"Nariz\", 19: \"Padrino\", 20: \"Película\", 21: \"Persona\", 22: \"Primero\",\n",
        "               23: \"Quedar\", 24: \"Saber\", 25: \"Sangre\", 26: \"Señorita\", 27: \"Suerte\", 28: \"Toro\"}\n",
        "for i in range(len(X)):\n",
        "    y_true.append(indx_to_tag[ynew[i] + 1])\n",
        "    y_pred.append(indx_to_tag[np.argmax(y[i])+1])\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(18.0, 15.0))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/matriz.png')\n",
        "plt.clf()\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure(figsize=(18.0, 15.0))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/matriz_norm.png')\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longitud conjunto = (1131, 2604, 2)\n",
            "Confusion matrix, without normalization\n",
            "[[32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0 49  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0 50  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0 44  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  1  0  0  0 40  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  1  0]\n",
            " [ 0  0  0  0  0  0 35  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 33  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 39  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 41  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 37  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 33  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 36  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0 43  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36  0  0  0  0  0  0  0\n",
            "   0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 42  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0 35  0  0  0  0\n",
            "   1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0 38  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 44  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45  0\n",
            "   1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 43\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  35  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0 42  0  0]\n",
            " [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0 28  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0 48]]\n",
            "Normalized confusion matrix\n",
            "[[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.98 0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.96 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.98 0.   0.   0.   0.   0.   0.02 0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.02 0.   0.   0.   0.95 0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.97 0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.03 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.94 0.   0.   0.   0.   0.06 0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.98 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.97 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.03 0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.03 0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.95 0.   0.   0.   0.   0.03 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.07 0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.93 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.02 0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.96 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.04 0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.94 0.   0.02 0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.03 0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.03 0.   0.   0.   0.   0.   0.   0.   0.   0.93 0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1pvORMJgtek",
        "colab_type": "text"
      },
      "source": [
        "### Matriz de confusión sin normalizar y normalizada, respectivamente\n",
        "\n",
        "![matriz](https://drive.google.com/uc?id=1oK-1piVnWd_tmZOLcG-hfCfe3L_d3Fdb)\n",
        "![normalizada](https://drive.google.com/uc?id=1MsvHezBaScuAjwZI3mwR60NpkDJtiPLm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyHWfRKvgnS8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd7-pFw2YYyJ",
        "colab_type": "text"
      },
      "source": [
        "### **Entrenamiento de la red sobre los videos anteriores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIbcaDLPodCZ",
        "colab_type": "code",
        "outputId": "33937d7f-d86c-4e46-bdf6-d884de083f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from numpy.random import seed\n",
        "seed(1996)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(1996)\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import  TimeseriesGenerator\n",
        "from keras.layers import LSTM, Conv1D, MaxPool1D, Flatten, CuDNNGRU, CuDNNLSTM, Flatten\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import keras as keras\n",
        "from keras.optimizers import  rmsprop\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/trainX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/y_train.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/testX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/y_test.npy')\n",
        "\n",
        "tam_input = 30\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(LSTM(15, return_sequences=True, input_shape=(tam_input, X_train.shape[1])))\n",
        "model.add(LSTM(15, input_shape=(tam_input, X_train.shape[1])))\n",
        "model.add(Dense(28, activation='softmax'))\n",
        "\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(CuDNNLSTM(10, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Conv1D(filters=15, kernel_size=15 ))\n",
        "model.add(MaxPool1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(28, activation='softmax'))\n",
        "opt = rmsprop(lr=0.001, rho=0.9)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "print(model.summary())\n",
        "batch_size = 256\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('model_lstm2{epoch:08d}.h5', period=350)\n",
        "history = model.fit(x=X_train, y= Y_train, epochs=150, batch_size=batch_size, validation_data=(X_test, Y_test) )\n",
        "\n",
        "model.save('/content/gdrive/My Drive/openpose_lstm/version_anterior/modelo_def.h5')\n",
        "validation_acc = np.amax(history.history['val_acc'])\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/version_anterior/acc2.png')\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/version_anterior/loss2.png')\n",
        "\n",
        "print('Best validation acc of epoch:', validation_acc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 21948, 10)         560       \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 21948, 10)         880       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 21934, 15)         2265      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 10967, 15)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 164505)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                4606168   \n",
            "=================================================================\n",
            "Total params: 4,609,873\n",
            "Trainable params: 4,609,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 8960 samples, validate on 1120 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "8960/8960 [==============================] - 82s 9ms/step - loss: 3.0837 - acc: 0.2180 - val_loss: 2.5667 - val_acc: 0.3205\n",
            "Epoch 2/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.1385 - acc: 0.4121 - val_loss: 1.7937 - val_acc: 0.4893\n",
            "Epoch 3/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.5930 - acc: 0.5405 - val_loss: 1.4939 - val_acc: 0.5384\n",
            "Epoch 4/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.3578 - acc: 0.6083 - val_loss: 1.6057 - val_acc: 0.5295\n",
            "Epoch 5/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 1.1675 - acc: 0.6609 - val_loss: 0.9978 - val_acc: 0.7134\n",
            "Epoch 6/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.9407 - acc: 0.7176 - val_loss: 0.7914 - val_acc: 0.7768\n",
            "Epoch 7/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.7416 - acc: 0.7738 - val_loss: 1.1179 - val_acc: 0.6732\n",
            "Epoch 8/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.6264 - acc: 0.8166 - val_loss: 0.6914 - val_acc: 0.7866\n",
            "Epoch 9/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.4791 - acc: 0.8566 - val_loss: 0.7381 - val_acc: 0.8000\n",
            "Epoch 10/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.4123 - acc: 0.8766 - val_loss: 0.3282 - val_acc: 0.9268\n",
            "Epoch 11/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.2860 - acc: 0.9164 - val_loss: 0.7624 - val_acc: 0.8036\n",
            "Epoch 12/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.3081 - acc: 0.9133 - val_loss: 0.3968 - val_acc: 0.8893\n",
            "Epoch 13/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.1670 - acc: 0.9529 - val_loss: 0.7984 - val_acc: 0.7812\n",
            "Epoch 14/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.1590 - acc: 0.9596 - val_loss: 0.1512 - val_acc: 0.9777\n",
            "Epoch 15/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.1306 - acc: 0.9676 - val_loss: 0.1503 - val_acc: 0.9813\n",
            "Epoch 16/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.1311 - acc: 0.9658 - val_loss: 0.1517 - val_acc: 0.9795\n",
            "Epoch 17/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.1113 - acc: 0.9675 - val_loss: 0.1996 - val_acc: 0.9625\n",
            "Epoch 18/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.1171 - acc: 0.9709 - val_loss: 0.1161 - val_acc: 0.9893\n",
            "Epoch 19/150\n",
            "8960/8960 [==============================] - 68s 8ms/step - loss: 0.0648 - acc: 0.9808 - val_loss: 0.1780 - val_acc: 0.9741\n",
            "Epoch 20/150\n",
            "8960/8960 [==============================] - 68s 8ms/step - loss: 0.0444 - acc: 0.9893 - val_loss: 0.2015 - val_acc: 0.9696\n",
            "Epoch 21/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.0898 - acc: 0.9772 - val_loss: 0.3915 - val_acc: 0.9152\n",
            "Epoch 22/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0409 - acc: 0.9900 - val_loss: 0.1225 - val_acc: 0.9866\n",
            "Epoch 23/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.0700 - acc: 0.9843 - val_loss: 0.1609 - val_acc: 0.9741\n",
            "Epoch 24/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.0506 - acc: 0.9863 - val_loss: 0.2262 - val_acc: 0.9536\n",
            "Epoch 25/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0684 - acc: 0.9814 - val_loss: 0.3940 - val_acc: 0.9080\n",
            "Epoch 26/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0425 - acc: 0.9881 - val_loss: 0.1196 - val_acc: 0.9902\n",
            "Epoch 27/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0630 - acc: 0.9840 - val_loss: 0.1127 - val_acc: 0.9884\n",
            "Epoch 28/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0225 - acc: 0.9948 - val_loss: 0.1270 - val_acc: 0.9857\n",
            "Epoch 29/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0691 - acc: 0.9843 - val_loss: 0.1220 - val_acc: 0.9866\n",
            "Epoch 30/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1193 - val_acc: 0.9866\n",
            "Epoch 31/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0587 - acc: 0.9859 - val_loss: 0.1144 - val_acc: 0.9884\n",
            "Epoch 32/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 9.9177e-04 - acc: 0.9999 - val_loss: 0.1135 - val_acc: 0.9875\n",
            "Epoch 33/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0728 - acc: 0.9849 - val_loss: 0.1312 - val_acc: 0.9866\n",
            "Epoch 34/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 8.0289e-04 - acc: 0.9999 - val_loss: 0.1226 - val_acc: 0.9875\n",
            "Epoch 35/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0606 - acc: 0.9876 - val_loss: 0.1254 - val_acc: 0.9884\n",
            "Epoch 36/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 6.2596e-04 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.9884\n",
            "Epoch 37/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0636 - acc: 0.9866 - val_loss: 0.1186 - val_acc: 0.9902\n",
            "Epoch 38/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.1195 - val_acc: 0.9866\n",
            "Epoch 39/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0507 - acc: 0.9915 - val_loss: 0.5134 - val_acc: 0.8804\n",
            "Epoch 40/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0169 - acc: 0.9951 - val_loss: 0.1314 - val_acc: 0.9875\n",
            "Epoch 41/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.0956e-04 - acc: 1.0000 - val_loss: 0.1358 - val_acc: 0.9875\n",
            "Epoch 42/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0640 - acc: 0.9881 - val_loss: 0.1506 - val_acc: 0.9866\n",
            "Epoch 43/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.3072 - val_acc: 0.9411\n",
            "Epoch 44/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0245 - acc: 0.9951 - val_loss: 0.1348 - val_acc: 0.9857\n",
            "Epoch 45/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0594 - acc: 0.9892 - val_loss: 0.1592 - val_acc: 0.9866\n",
            "Epoch 46/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 9.4160e-04 - acc: 0.9997 - val_loss: 0.1458 - val_acc: 0.9875\n",
            "Epoch 47/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.1280e-04 - acc: 1.0000 - val_loss: 0.1493 - val_acc: 0.9875\n",
            "Epoch 48/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0431 - acc: 0.9897 - val_loss: 0.1403 - val_acc: 0.9884\n",
            "Epoch 49/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 1.4268e-04 - acc: 1.0000 - val_loss: 0.1380 - val_acc: 0.9884\n",
            "Epoch 50/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 7.6750e-05 - acc: 1.0000 - val_loss: 0.2113 - val_acc: 0.9768\n",
            "Epoch 51/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0655 - acc: 0.9868 - val_loss: 0.1456 - val_acc: 0.9875\n",
            "Epoch 52/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.1203e-04 - acc: 1.0000 - val_loss: 0.1298 - val_acc: 0.9893\n",
            "Epoch 53/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0368 - acc: 0.9912 - val_loss: 0.1429 - val_acc: 0.9857\n",
            "Epoch 54/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0844 - acc: 0.9858 - val_loss: 0.1409 - val_acc: 0.9857\n",
            "Epoch 55/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1279 - val_acc: 0.9884\n",
            "Epoch 56/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 9.3395e-05 - acc: 1.0000 - val_loss: 0.1288 - val_acc: 0.9884\n",
            "Epoch 57/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0530 - acc: 0.9888 - val_loss: 0.2146 - val_acc: 0.9759\n",
            "Epoch 58/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1430 - val_acc: 0.9875\n",
            "Epoch 59/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.0618e-04 - acc: 1.0000 - val_loss: 0.1430 - val_acc: 0.9884\n",
            "Epoch 60/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0625 - acc: 0.9883 - val_loss: 0.1369 - val_acc: 0.9839\n",
            "Epoch 61/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 6.4322e-04 - acc: 0.9998 - val_loss: 0.1305 - val_acc: 0.9875\n",
            "Epoch 62/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 6.8097e-05 - acc: 1.0000 - val_loss: 0.1312 - val_acc: 0.9875\n",
            "Epoch 63/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0428 - acc: 0.9913 - val_loss: 0.1389 - val_acc: 0.9857\n",
            "Epoch 64/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0277 - acc: 0.9948 - val_loss: 0.5676 - val_acc: 0.9107\n",
            "Epoch 65/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.1390 - val_acc: 0.9866\n",
            "Epoch 66/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 8.8363e-05 - acc: 1.0000 - val_loss: 0.1414 - val_acc: 0.9866\n",
            "Epoch 67/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.1497 - val_acc: 0.9875\n",
            "Epoch 68/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 7.2864e-05 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 0.9875\n",
            "Epoch 69/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 3.3444e-05 - acc: 1.0000 - val_loss: 0.1469 - val_acc: 0.9875\n",
            "Epoch 70/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0440 - acc: 0.9926 - val_loss: 0.2766 - val_acc: 0.9545\n",
            "Epoch 71/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1161 - val_acc: 0.9884\n",
            "Epoch 72/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 3.6100e-05 - acc: 1.0000 - val_loss: 0.1183 - val_acc: 0.9884\n",
            "Epoch 73/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 1.5411e-05 - acc: 1.0000 - val_loss: 0.1192 - val_acc: 0.9893\n",
            "Epoch 74/150\n",
            "8960/8960 [==============================] - 68s 8ms/step - loss: 0.0655 - acc: 0.9895 - val_loss: 0.1475 - val_acc: 0.9866\n",
            "Epoch 75/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.6310e-05 - acc: 1.0000 - val_loss: 0.1446 - val_acc: 0.9866\n",
            "Epoch 76/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.5739e-05 - acc: 1.0000 - val_loss: 0.1427 - val_acc: 0.9866\n",
            "Epoch 77/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0409 - acc: 0.9922 - val_loss: 0.1306 - val_acc: 0.9866\n",
            "Epoch 78/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1310 - val_acc: 0.9875\n",
            "Epoch 79/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.7523e-05 - acc: 1.0000 - val_loss: 0.1282 - val_acc: 0.9875\n",
            "Epoch 80/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.0431e-05 - acc: 1.0000 - val_loss: 0.1297 - val_acc: 0.9875\n",
            "Epoch 81/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0561 - acc: 0.9894 - val_loss: 0.1575 - val_acc: 0.9866\n",
            "Epoch 82/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 5.6114e-05 - acc: 1.0000 - val_loss: 0.1571 - val_acc: 0.9875\n",
            "Epoch 83/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.8485e-05 - acc: 1.0000 - val_loss: 0.1556 - val_acc: 0.9875\n",
            "Epoch 84/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 7.6115e-06 - acc: 1.0000 - val_loss: 0.1520 - val_acc: 0.9875\n",
            "Epoch 85/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0370 - acc: 0.9934 - val_loss: 0.1400 - val_acc: 0.9839\n",
            "Epoch 86/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0265 - acc: 0.9950 - val_loss: 0.1642 - val_acc: 0.9857\n",
            "Epoch 87/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.9186e-04 - acc: 0.9999 - val_loss: 0.1497 - val_acc: 0.9866\n",
            "Epoch 88/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 1.5465e-05 - acc: 1.0000 - val_loss: 0.1481 - val_acc: 0.9866\n",
            "Epoch 89/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 8.5898e-06 - acc: 1.0000 - val_loss: 0.2070 - val_acc: 0.9732\n",
            "Epoch 90/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0365 - acc: 0.9922 - val_loss: 0.1371 - val_acc: 0.9866\n",
            "Epoch 91/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.9847e-05 - acc: 1.0000 - val_loss: 0.1369 - val_acc: 0.9866\n",
            "Epoch 92/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 1.2752e-05 - acc: 1.0000 - val_loss: 0.1315 - val_acc: 0.9866\n",
            "Epoch 93/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 4.9572e-06 - acc: 1.0000 - val_loss: 0.1352 - val_acc: 0.9866\n",
            "Epoch 94/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0214 - acc: 0.9950 - val_loss: 0.1328 - val_acc: 0.9866\n",
            "Epoch 95/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 3.2256e-05 - acc: 1.0000 - val_loss: 0.1311 - val_acc: 0.9866\n",
            "Epoch 96/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 8.9470e-06 - acc: 1.0000 - val_loss: 0.1319 - val_acc: 0.9866\n",
            "Epoch 97/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0286 - acc: 0.9937 - val_loss: 0.1208 - val_acc: 0.9893\n",
            "Epoch 98/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.6960e-04 - acc: 1.0000 - val_loss: 0.1152 - val_acc: 0.9893\n",
            "Epoch 99/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.9445e-05 - acc: 1.0000 - val_loss: 0.1130 - val_acc: 0.9875\n",
            "Epoch 100/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 5.3430e-06 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 0.9875\n",
            "Epoch 101/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.1531e-06 - acc: 1.0000 - val_loss: 0.1136 - val_acc: 0.9875\n",
            "Epoch 102/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0541 - acc: 0.9922 - val_loss: 0.1191 - val_acc: 0.9893\n",
            "Epoch 103/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.4884e-05 - acc: 1.0000 - val_loss: 0.1193 - val_acc: 0.9893\n",
            "Epoch 104/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 5.3257e-06 - acc: 1.0000 - val_loss: 0.1160 - val_acc: 0.9893\n",
            "Epoch 105/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.5562e-06 - acc: 1.0000 - val_loss: 0.1156 - val_acc: 0.9893\n",
            "Epoch 106/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0397 - acc: 0.9921 - val_loss: 0.1128 - val_acc: 0.9893\n",
            "Epoch 107/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.5839e-05 - acc: 1.0000 - val_loss: 0.1129 - val_acc: 0.9875\n",
            "Epoch 108/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 8.0022e-06 - acc: 1.0000 - val_loss: 0.1143 - val_acc: 0.9875\n",
            "Epoch 109/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.5840e-06 - acc: 1.0000 - val_loss: 0.1158 - val_acc: 0.9875\n",
            "Epoch 110/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.2134e-06 - acc: 1.0000 - val_loss: 0.1201 - val_acc: 0.9875\n",
            "Epoch 111/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0242 - acc: 0.9948 - val_loss: 0.1381 - val_acc: 0.9884\n",
            "Epoch 112/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.5630e-05 - acc: 1.0000 - val_loss: 0.1415 - val_acc: 0.9866\n",
            "Epoch 113/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 5.1830e-06 - acc: 1.0000 - val_loss: 0.1401 - val_acc: 0.9866\n",
            "Epoch 114/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.0078e-06 - acc: 1.0000 - val_loss: 0.1397 - val_acc: 0.9866\n",
            "Epoch 115/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 8.6371e-07 - acc: 1.0000 - val_loss: 0.1412 - val_acc: 0.9866\n",
            "Epoch 116/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 4.0022e-07 - acc: 1.0000 - val_loss: 0.1415 - val_acc: 0.9866\n",
            "Epoch 117/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0479 - acc: 0.9921 - val_loss: 0.1322 - val_acc: 0.9884\n",
            "Epoch 118/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.3268e-05 - acc: 1.0000 - val_loss: 0.1338 - val_acc: 0.9866\n",
            "Epoch 119/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.3964e-06 - acc: 1.0000 - val_loss: 0.1351 - val_acc: 0.9884\n",
            "Epoch 120/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.3432e-06 - acc: 1.0000 - val_loss: 0.1349 - val_acc: 0.9884\n",
            "Epoch 121/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 5.9463e-07 - acc: 1.0000 - val_loss: 0.1360 - val_acc: 0.9884\n",
            "Epoch 122/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.0446e-07 - acc: 1.0000 - val_loss: 0.1352 - val_acc: 0.9884\n",
            "Epoch 123/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0226 - acc: 0.9946 - val_loss: 0.1714 - val_acc: 0.9848\n",
            "Epoch 124/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1497 - val_acc: 0.9875\n",
            "Epoch 125/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 6.4854e-06 - acc: 1.0000 - val_loss: 0.1477 - val_acc: 0.9875\n",
            "Epoch 126/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 1.8262e-06 - acc: 1.0000 - val_loss: 0.1466 - val_acc: 0.9875\n",
            "Epoch 127/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 5.9781e-07 - acc: 1.0000 - val_loss: 0.1460 - val_acc: 0.9875\n",
            "Epoch 128/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 2.7239e-07 - acc: 1.0000 - val_loss: 0.1460 - val_acc: 0.9875\n",
            "Epoch 129/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.8181e-07 - acc: 1.0000 - val_loss: 0.1511 - val_acc: 0.9893\n",
            "Epoch 130/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 0.0308 - acc: 0.9945 - val_loss: 0.1477 - val_acc: 0.9893\n",
            "Epoch 131/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.1575e-05 - acc: 1.0000 - val_loss: 0.1516 - val_acc: 0.9893\n",
            "Epoch 132/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.6898e-06 - acc: 1.0000 - val_loss: 0.1522 - val_acc: 0.9893\n",
            "Epoch 133/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 6.2066e-07 - acc: 1.0000 - val_loss: 0.1531 - val_acc: 0.9893\n",
            "Epoch 134/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.8754e-07 - acc: 1.0000 - val_loss: 0.1542 - val_acc: 0.9893\n",
            "Epoch 135/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.9263e-07 - acc: 1.0000 - val_loss: 0.1603 - val_acc: 0.9893\n",
            "Epoch 136/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0202 - acc: 0.9952 - val_loss: 0.1472 - val_acc: 0.9884\n",
            "Epoch 137/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 6.4613e-06 - acc: 1.0000 - val_loss: 0.1472 - val_acc: 0.9884\n",
            "Epoch 138/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 2.1487e-06 - acc: 1.0000 - val_loss: 0.1468 - val_acc: 0.9884\n",
            "Epoch 139/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 6.9540e-07 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9884\n",
            "Epoch 140/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 3.1110e-07 - acc: 1.0000 - val_loss: 0.1445 - val_acc: 0.9884\n",
            "Epoch 141/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 1.6955e-07 - acc: 1.0000 - val_loss: 0.1449 - val_acc: 0.9875\n",
            "Epoch 142/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 1.3645e-07 - acc: 1.0000 - val_loss: 0.1459 - val_acc: 0.9875\n",
            "Epoch 143/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 0.0239 - acc: 0.9954 - val_loss: 0.1495 - val_acc: 0.9875\n",
            "Epoch 144/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 3.6095e-06 - acc: 1.0000 - val_loss: 0.1467 - val_acc: 0.9875\n",
            "Epoch 145/150\n",
            "8960/8960 [==============================] - 66s 7ms/step - loss: 1.0674e-06 - acc: 1.0000 - val_loss: 0.1453 - val_acc: 0.9875\n",
            "Epoch 146/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 4.0470e-07 - acc: 1.0000 - val_loss: 0.1448 - val_acc: 0.9875\n",
            "Epoch 147/150\n",
            "8960/8960 [==============================] - 67s 7ms/step - loss: 2.0381e-07 - acc: 1.0000 - val_loss: 0.1445 - val_acc: 0.9875\n",
            "Epoch 148/150\n",
            "8960/8960 [==============================] - 68s 8ms/step - loss: 1.4596e-07 - acc: 1.0000 - val_loss: 0.1458 - val_acc: 0.9875\n",
            "Epoch 149/150\n",
            "8960/8960 [==============================] - 67s 8ms/step - loss: 0.0271 - acc: 0.9959 - val_loss: 0.1404 - val_acc: 0.9893\n",
            "Epoch 150/150\n",
            "8960/8960 [==============================] - 69s 8ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1371 - val_acc: 0.9893\n",
            "Best validation acc of epoch: 0.9901785731315613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swsnu7jJepjc",
        "colab_type": "text"
      },
      "source": [
        "### Gráficas de pérdida y de precisión del modelo, respectivamente\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1ge0VWY8u09DpwboUz9bXUHROG6xUk2um)\n",
        "![alt text](https://drive.google.com/uc?id=1TJlJ0DC05g5zLXNCNkTOP4B2VV6pSKYQ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhCm2LnZXbaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2afc1a3c-b520-4fda-bd3a-9b3654656893"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "# generate 2d classification dataset\n",
        "X, y = np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/devX.npy'), np.load('/content/gdrive/My Drive/openpose_lstm/version_anterior/y_dev.npy')\n",
        "\n",
        "# define and fit the final model\n",
        "\n",
        "model = load_model('/content/gdrive/My Drive/openpose_lstm/version_anterior/modelo_def.h5')\n",
        "\n",
        "ynew = model.predict_classes(X)\n",
        "# show the inputs and predicted outputs\n",
        "\n",
        "print(\"Longitud conjunto = \" + str(X.shape))\n",
        "y_pred = []\n",
        "y_true = []\n",
        "class_names =  np.array([\"Agua\", \"Alto\", \"Apagar\", \"Ayudar\", \"Campo\", \"Crecer\", \"Cuerpo\", \"Despedir\", \"Dinero\", \"Escribir\", \"Foto\", \"Fruto\", \"Hacer\", \"LavarCara\", \"Madera\", \"Miedo\", \"Morder\", \"Nariz\", \"Padrino\",\"Película\", \"Persona\", \"Primero\", \"Quedar\", \"Saber\", \"Sangre\", \"Señorita\", \"Suerte\", \"Toro\"])\n",
        "tag_to_indx = {\"Agua\": 1, \"Alto\": 2, \"Apagar\": 3, \"Ayudar\": 4, \"Campo\": 5, \"Crecer\": 6, \"Cuerpo\": 7, \"Despedir\": 8,\n",
        "               \"Dinero\": 9, \"Escribir\": 10, \"Foto\": 11, \"Fruto\": 12, \"Hacer\": 13, \"LavarCara\": 14, \"Madera\": 15,\n",
        "               \"Miedo\": 16, \"Morder\": 17, \"Nariz\": 18, \"Padrino\": 19, \"Película\": 20, \"Persona\": 21, \"Primero\": 22,\n",
        "               \"Quedar\": 23, \"Saber\": 24, \"Sangre\": 25, \"Señorita\": 26, \"Suerte\": 27, \"Toro\": 28}\n",
        "indx_to_tag = {1: \"Agua\", 2: \"Alto\", 3: \"Apagar\", 4: \"Ayudar\", 5: \"Campo\", 6: \"Crecer\", 7: \"Cuerpo\", 8: \"Despedir\",\n",
        "               9: \"Dinero\", 10: \"Escribir\", 11: \"Foto\", 12: \"Fruto\", 13: \"Hacer\", 14: \"LavarCara\", 15: \"Madera\",\n",
        "               16: \"Miedo\", 17: \"Morder\", 18: \"Nariz\", 19: \"Padrino\", 20: \"Película\", 21: \"Persona\", 22: \"Primero\",\n",
        "               23: \"Quedar\", 24: \"Saber\", 25: \"Sangre\", 26: \"Señorita\", 27: \"Suerte\", 28: \"Toro\"}\n",
        "for i in range(len(X)):\n",
        "    y_true.append(indx_to_tag[ynew[i] + 1])\n",
        "    y_pred.append(indx_to_tag[np.argmax(y[i])+1])\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(18.0, 15.0))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/version_anterior/matriz.png')\n",
        "plt.clf()\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure(figsize=(18.0, 15.0))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.savefig('/content/gdrive/My Drive/openpose_lstm/version_anterior/matriz_norm.png')\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longitud conjunto = (1120, 21948, 2)\n",
            "Confusion matrix, without normalization\n",
            "[[42  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0 41  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0 33  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0 42  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0 37  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0 45  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 44  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 42  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 44  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 37  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 32  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 42  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 38  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 38  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 41  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 47  0  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36  0\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 43\n",
            "   0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  45  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0 42  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0 37  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0 39]]\n",
            "Normalized confusion matrix\n",
            "[[1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.   0.   0.\n",
            "  0.   0.   0.   0.02 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
            " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh_iBnaGhYUd",
        "colab_type": "text"
      },
      "source": [
        "### Matriz de confusión sin normalizar y normalizada, respectivamente\n",
        "\n",
        "![matriz](https://drive.google.com/uc?id=1-3AykeOW4t54hak_LT9-6Bvj3h2Dt8xt)\n",
        "![normalizada](https://drive.google.com/uc?id=1-6b8dw5cumKguyhmrXykStatWwH1Zo7W)"
      ]
    }
  ]
}