{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-hp/LSTM/blob/master/LSTM_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab3eCGObBInb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "636205db-90ca-4d50-a9bf-b8ab649fc046"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "with open('salida_300_propio.txt') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "print(\"Salida \")\n",
        "print(data[-150:])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida \n",
            "['35/60 [================>.............] - ETA: 8:53 - loss: 0.0018 - acc: 0.9995\\n', '36/60 [=================>............] - ETA: 8:31 - loss: 0.0018 - acc: 0.9995\\n', '37/60 [=================>............] - ETA: 8:10 - loss: 0.0018 - acc: 0.9995\\n', '38/60 [==================>...........] - ETA: 7:48 - loss: 0.0018 - acc: 0.9995\\n', '39/60 [==================>...........] - ETA: 7:26 - loss: 0.0017 - acc: 0.9996\\n', '40/60 [===================>..........] - ETA: 7:05 - loss: 0.0017 - acc: 0.9996\\n', '41/60 [===================>..........] - ETA: 6:43 - loss: 0.0017 - acc: 0.9996\\n', '42/60 [====================>.........] - ETA: 6:22 - loss: 0.0017 - acc: 0.9996\\n', '43/60 [====================>.........] - ETA: 6:00 - loss: 0.0017 - acc: 0.9996\\n', '44/60 [=====================>........] - ETA: 5:39 - loss: 0.0017 - acc: 0.9996\\n', '45/60 [=====================>........] - ETA: 5:17 - loss: 0.0017 - acc: 0.9996\\n', '46/60 [======================>.......] - ETA: 4:56 - loss: 0.0017 - acc: 0.9996\\n', '47/60 [======================>.......] - ETA: 4:35 - loss: 0.0017 - acc: 0.9996\\n', '48/60 [=======================>......] - ETA: 4:14 - loss: 0.0017 - acc: 0.9996\\n', '49/60 [=======================>......] - ETA: 3:53 - loss: 0.0017 - acc: 0.9996\\n', '50/60 [========================>.....] - ETA: 3:32 - loss: 0.0017 - acc: 0.9996\\n', '51/60 [========================>.....] - ETA: 3:10 - loss: 0.0017 - acc: 0.9996\\n', '52/60 [=========================>....] - ETA: 2:49 - loss: 0.0017 - acc: 0.9996\\n', '53/60 [=========================>....] - ETA: 2:28 - loss: 0.0017 - acc: 0.9996\\n', '54/60 [==========================>...] - ETA: 2:07 - loss: 0.0016 - acc: 0.9996\\n', '55/60 [==========================>...] - ETA: 1:46 - loss: 0.0016 - acc: 0.9996\\n', '56/60 [===========================>..] - ETA: 1:24 - loss: 0.0016 - acc: 0.9996\\n', '57/60 [===========================>..] - ETA: 1:03 - loss: 0.0017 - acc: 0.9996\\n', '58/60 [============================>.] - ETA: 42s - loss: 0.0016 - acc: 0.9996 \\n', '59/60 [============================>.] - ETA: 21s - loss: 0.0017 - acc: 0.9996\\n', '60/60 [==============================] - 1274s 21s/step - loss: 0.0016 - acc: 0.9996 - val_loss: 9.7976 - val_acc: 0.0345\\n', 'Epoch 35/1000\\n', '\\n', ' 1/60 [..............................] - ETA: 21:50 - loss: 0.0011 - acc: 1.0000\\n', ' 2/60 [>.............................] - ETA: 21:00 - loss: 0.0011 - acc: 1.0000\\n', ' 3/60 [>.............................] - ETA: 20:27 - loss: 9.3148e-04 - acc: 1.0000\\n', ' 4/60 [=>............................] - ETA: 20:05 - loss: 8.4129e-04 - acc: 1.0000\\n', ' 5/60 [=>............................] - ETA: 19:39 - loss: 7.2787e-04 - acc: 1.0000\\n', ' 6/60 [==>...........................] - ETA: 19:11 - loss: 0.0010 - acc: 0.9998    \\n', ' 7/60 [==>...........................] - ETA: 18:49 - loss: 9.5353e-04 - acc: 0.9998\\n', ' 8/60 [===>..........................] - ETA: 18:36 - loss: 9.5255e-04 - acc: 0.9998\\n', ' 9/60 [===>..........................] - ETA: 18:14 - loss: 9.1321e-04 - acc: 0.9999\\n', '10/60 [====>.........................] - ETA: 17:54 - loss: 8.9050e-04 - acc: 0.9999\\n', '11/60 [====>.........................] - ETA: 17:31 - loss: 8.3343e-04 - acc: 0.9999\\n', '12/60 [=====>........................] - ETA: 17:02 - loss: 7.9687e-04 - acc: 0.9999\\n', '13/60 [=====>........................] - ETA: 16:39 - loss: 7.8315e-04 - acc: 0.9999\\n', '14/60 [======>.......................] - ETA: 16:17 - loss: 7.5903e-04 - acc: 0.9999\\n', '15/60 [======>.......................] - ETA: 15:57 - loss: 7.3980e-04 - acc: 0.9999\\n', '16/60 [=======>......................] - ETA: 15:33 - loss: 8.4073e-04 - acc: 0.9998\\n', '17/60 [=======>......................] - ETA: 15:12 - loss: 8.4421e-04 - acc: 0.9998\\n', '18/60 [========>.....................] - ETA: 14:48 - loss: 0.0010 - acc: 0.9997    \\n', '19/60 [========>.....................] - ETA: 14:25 - loss: 0.0010 - acc: 0.9997\\n', '20/60 [=========>....................] - ETA: 14:06 - loss: 0.0010 - acc: 0.9997\\n', '21/60 [=========>....................] - ETA: 13:48 - loss: 9.8858e-04 - acc: 0.9997\\n', '22/60 [==========>...................] - ETA: 13:27 - loss: 9.7578e-04 - acc: 0.9998\\n', '23/60 [==========>...................] - ETA: 13:04 - loss: 9.4760e-04 - acc: 0.9998\\n', '24/60 [===========>..................] - ETA: 12:43 - loss: 9.5227e-04 - acc: 0.9998\\n', '25/60 [===========>..................] - ETA: 12:21 - loss: 9.4001e-04 - acc: 0.9998\\n', '26/60 [============>.................] - ETA: 11:59 - loss: 9.1726e-04 - acc: 0.9998\\n', '27/60 [============>.................] - ETA: 11:38 - loss: 8.9844e-04 - acc: 0.9998\\n', '28/60 [=============>................] - ETA: 11:17 - loss: 9.3819e-04 - acc: 0.9998\\n', '29/60 [=============>................] - ETA: 10:55 - loss: 9.3114e-04 - acc: 0.9998\\n', '30/60 [==============>...............] - ETA: 10:34 - loss: 9.2717e-04 - acc: 0.9998\\n', '31/60 [==============>...............] - ETA: 10:12 - loss: 9.1531e-04 - acc: 0.9998\\n', '32/60 [===============>..............] - ETA: 9:50 - loss: 9.0369e-04 - acc: 0.9998 \\n', '33/60 [===============>..............] - ETA: 9:29 - loss: 9.0417e-04 - acc: 0.9998\\n', '34/60 [================>.............] - ETA: 9:08 - loss: 8.8512e-04 - acc: 0.9998\\n', '35/60 [================>.............] - ETA: 8:47 - loss: 8.9437e-04 - acc: 0.9998\\n', '36/60 [=================>............] - ETA: 8:25 - loss: 8.8556e-04 - acc: 0.9998\\n', '37/60 [=================>............] - ETA: 8:03 - loss: 8.6896e-04 - acc: 0.9998\\n', '38/60 [==================>...........] - ETA: 7:41 - loss: 8.6986e-04 - acc: 0.9998\\n', '39/60 [==================>...........] - ETA: 7:21 - loss: 8.6553e-04 - acc: 0.9998\\n', '40/60 [===================>..........] - ETA: 7:00 - loss: 8.5697e-04 - acc: 0.9998\\n', '41/60 [===================>..........] - ETA: 6:39 - loss: 8.4644e-04 - acc: 0.9998\\n', '42/60 [====================>.........] - ETA: 6:18 - loss: 8.3953e-04 - acc: 0.9998\\n', '43/60 [====================>.........] - ETA: 5:57 - loss: 8.3481e-04 - acc: 0.9998\\n', '44/60 [=====================>........] - ETA: 5:36 - loss: 8.3037e-04 - acc: 0.9998\\n', '45/60 [=====================>........] - ETA: 5:15 - loss: 8.2040e-04 - acc: 0.9998\\n', '46/60 [======================>.......] - ETA: 4:54 - loss: 9.0131e-04 - acc: 0.9998\\n', '47/60 [======================>.......] - ETA: 4:33 - loss: 8.9550e-04 - acc: 0.9998\\n', '48/60 [=======================>......] - ETA: 4:12 - loss: 8.9168e-04 - acc: 0.9998\\n', '49/60 [=======================>......] - ETA: 3:51 - loss: 9.2961e-04 - acc: 0.9998\\n', '50/60 [========================>.....] - ETA: 3:30 - loss: 9.2244e-04 - acc: 0.9998\\n', '51/60 [========================>.....] - ETA: 3:09 - loss: 9.2682e-04 - acc: 0.9998\\n', '52/60 [=========================>....] - ETA: 2:48 - loss: 9.2019e-04 - acc: 0.9998\\n', '53/60 [=========================>....] - ETA: 2:27 - loss: 9.2429e-04 - acc: 0.9998\\n', '54/60 [==========================>...] - ETA: 2:06 - loss: 9.1950e-04 - acc: 0.9998\\n', '55/60 [==========================>...] - ETA: 1:45 - loss: 9.2681e-04 - acc: 0.9998\\n', '56/60 [===========================>..] - ETA: 1:24 - loss: 9.4382e-04 - acc: 0.9998\\n', '57/60 [===========================>..] - ETA: 1:03 - loss: 9.3826e-04 - acc: 0.9998\\n', '58/60 [============================>.] - ETA: 42s - loss: 9.3503e-04 - acc: 0.9998 \\n', '59/60 [============================>.] - ETA: 21s - loss: 9.3166e-04 - acc: 0.9998\\n', '60/60 [==============================] - 1266s 21s/step - loss: 9.3981e-04 - acc: 0.9998 - val_loss: 10.1737 - val_acc: 0.0000e+00\\n', 'Epoch 36/1000\\n', '\\n', ' 1/60 [..............................] - ETA: 21:50 - loss: 5.8077e-04 - acc: 1.0000\\n', ' 2/60 [>.............................] - ETA: 21:34 - loss: 7.7037e-04 - acc: 1.0000\\n', ' 3/60 [>.............................] - ETA: 20:47 - loss: 6.8646e-04 - acc: 1.0000\\n', ' 4/60 [=>............................] - ETA: 20:12 - loss: 6.9721e-04 - acc: 1.0000\\n', ' 5/60 [=>............................] - ETA: 19:38 - loss: 7.9188e-04 - acc: 1.0000\\n', ' 6/60 [==>...........................] - ETA: 19:18 - loss: 7.5448e-04 - acc: 1.0000\\n', ' 7/60 [==>...........................] - ETA: 18:58 - loss: 0.0010 - acc: 0.9998    \\n', ' 8/60 [===>..........................] - ETA: 18:44 - loss: 9.4068e-04 - acc: 0.9998\\n', ' 9/60 [===>..........................] - ETA: 18:23 - loss: 9.3621e-04 - acc: 0.9999\\n', '10/60 [====>.........................] - ETA: 17:58 - loss: 9.2289e-04 - acc: 0.9999\\n', '11/60 [====>.........................] - ETA: 17:36 - loss: 0.0011 - acc: 0.9998    \\n', '12/60 [=====>........................] - ETA: 17:09 - loss: 0.0011 - acc: 0.9998\\n', '13/60 [=====>........................] - ETA: 16:43 - loss: 0.0011 - acc: 0.9997\\n', '14/60 [======>.......................] - ETA: 16:20 - loss: 0.0012 - acc: 0.9996\\n', '15/60 [======>.......................] - ETA: 16:04 - loss: 0.0012 - acc: 0.9996\\n', '16/60 [=======>......................] - ETA: 15:40 - loss: 0.0012 - acc: 0.9997\\n', '17/60 [=======>......................] - ETA: 15:18 - loss: 0.0011 - acc: 0.9997\\n', '18/60 [========>.....................] - ETA: 14:58 - loss: 0.0011 - acc: 0.9997\\n', '19/60 [========>.....................] - ETA: 14:34 - loss: 0.0011 - acc: 0.9997\\n', '20/60 [=========>....................] - ETA: 14:12 - loss: 0.0011 - acc: 0.9997\\n', '21/60 [=========>....................] - ETA: 13:52 - loss: 0.0011 - acc: 0.9997\\n', '22/60 [==========>...................] - ETA: 13:32 - loss: 0.0011 - acc: 0.9998\\n', '23/60 [==========>...................] - ETA: 13:12 - loss: 0.0011 - acc: 0.9998\\n', '24/60 [===========>..................] - ETA: 12:49 - loss: 0.0010 - acc: 0.9998\\n', '25/60 [===========>..................] - ETA: 12:27 - loss: 0.0010 - acc: 0.9998\\n', '26/60 [============>.................] - ETA: 12:04 - loss: 0.0011 - acc: 0.9998\\n', '27/60 [============>.................] - ETA: 11:42 - loss: 0.0010 - acc: 0.9998\\n', '28/60 [=============>................] - ETA: 11:22 - loss: 0.0011 - acc: 0.9998\\n', '29/60 [=============>................] - ETA: 11:01 - loss: 0.0011 - acc: 0.9998\\n', '30/60 [==============>...............] - ETA: 10:39 - loss: 0.0011 - acc: 0.9998\\n', '31/60 [==============>...............] - ETA: 10:17 - loss: 0.0013 - acc: 0.9997\\n', '32/60 [===============>..............] - ETA: 9:55 - loss: 0.0013 - acc: 0.9997 \\n', '33/60 [===============>..............] - ETA: 9:33 - loss: 0.0013 - acc: 0.9998\\n', '34/60 [================>.............] - ETA: 9:12 - loss: 0.0012 - acc: 0.9998\\n', '35/60 [================>.............] - ETA: 8:51 - loss: 0.0012 - acc: 0.9998\\n', '36/60 [=================>............] - ETA: 8:30 - loss: 0.0012 - acc: 0.9998\\n', '37/60 [=================>............] - ETA: 8:09 - loss: 0.0012 - acc: 0.9998\\n', '38/60 [==================>...........] - ETA: 7:47 - loss: 0.0013 - acc: 0.9997\\n', '39/60 [==================>...........] - ETA: 7:26 - loss: 0.0012 - acc: 0.9997\\n', '40/60 [===================>..........] - ETA: 7:04 - loss: 0.0012 - acc: 0.9997\\n', '41/60 [===================>..........] - ETA: 6:43 - loss: 0.0012 - acc: 0.9997\\n', '42/60 [====================>.........] - ETA: 6:22 - loss: 0.0012 - acc: 0.9997\\n', '43/60 [====================>.........] - ETA: 6:01 - loss: 0.0012 - acc: 0.9998\\n', '44/60 [=====================>........] - ETA: 5:39 - loss: 0.0012 - acc: 0.9998\\n', '45/60 [=====================>........] - ETA: 5:18 - loss: 0.0012 - acc: 0.9998\\n', '46/60 [======================>.......] - ETA: 4:57 - loss: 0.0012 - acc: 0.9998\\n', '47/60 [======================>.......] - ETA: 4:35 - loss: 0.0012 - acc: 0.9998\\n', '48/60 [=======================>......] - ETA: 4:14 - loss: 0.0012 - acc: 0.9998\\n', '49/60 [=======================>......] - ETA: 3:53 - loss: 0.0012 - acc: 0.9998\\n', '50/60 [========================>.....] - ETA: 3:32 - loss: 0.0013 - acc: 0.9997\\n', '51/60 [========================>.....] - ETA: 3:11 - loss: 0.0013 - acc: 0.9997\\n', '52/60 [=========================>....] - ETA: 2:50 - loss: 0.0013 - acc: 0.9997\\n', '53/60 [=========================>....] - ETA: 2:28 - loss: 0.0013 - acc: 0.9997\\n', '54/60 [==========================>...] - ETA: 2:07 - loss: 0.0013 - acc: 0.9997\\n', '55/60 [==========================>...] - ETA: 1:46 - loss: 0.0013 - acc: 0.9997\\n', '56/60 [===========================>..] - ETA: 1:25 - loss: 0.0013 - acc: 0.9997\\n', '57/60 [===========================>..] - ETA: 1:03 - loss: 0.0013 - acc: 0.9997\\n', '58/60 [============================>.] - ETA: 42s - loss: 0.0013 - acc: 0.9997 \\n', '59/60 [============================>.] - ETA: 21s - loss: 0.0013 - acc: 0.9997\\n', '60/60 [==============================] - 1274s 21s/step - loss: 0.0013 - acc: 0.9997 - val_loss: 10.1613 - val_acc: 0.0345\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjE0lji7CoBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, SpatialDropout2D, SpatialDropout3D, Flatten, Reshape, Softmax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout, ConvLSTM2D, TimeDistributed\n",
        "from keras.utils import Sequence\n",
        "from keras.optimizers import Adam, SGD\n",
        "def normaliza(mayor, minimo, elem):\n",
        "    if mayor == 0:\n",
        "        return 0\n",
        "    if (((elem - minimo) / (mayor - minimo)) * 10) > 10:\n",
        "        return 10\n",
        "    return ((elem - minimo) / (mayor - minimo)) * 10\n",
        "\n",
        "\n",
        "def procesaEntrada(arr, h1_m=101, w1_m=199, h2_m=199, w2_m=199):\n",
        "    #posiciones = []\n",
        "    if len(arr) > 0:\n",
        "        h1 = arr[:, 2]\n",
        "        w1 = arr[:, 3]\n",
        "        h2 = arr[:, 4]\n",
        "        w2 = arr[:, 5]\n",
        "\n",
        "\n",
        "        nv_h1 = copy.deepcopy(h1)\n",
        "        nv_w1 = copy.deepcopy(w1)\n",
        "        nv_h2 = copy.deepcopy(h2)\n",
        "        nv_w2 = copy.deepcopy(w2)\n",
        "        idx = 0\n",
        "        for a, b, c, d in zip(w1, h1, w2, h2):\n",
        "        #    if len(w1) < 5 :\n",
        "        #        posiciones.append(idx)\n",
        "        #        continue\n",
        "            nv_h1[idx] = normaliza(h1_m, 0, b)\n",
        "            nv_w1[idx] = normaliza(w1_m, 0, a)\n",
        "            nv_h2[idx] = normaliza(h2_m, 0, d)\n",
        "            nv_w2[idx] = normaliza(w2_m, 0, c)\n",
        "            idx += 1\n",
        "        return np.column_stack((np.array(arr[:, 0]), np.array(arr[:, 1]), np.array(nv_h1), np.array(nv_w1),\n",
        "                                np.array(nv_h2), np.array(nv_w2)))\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "def convierteATexto(texto):\n",
        "    cadena = np.array(np.zeros((texto.shape[0])), dtype = 'object')\n",
        "    for m in range(0, len(cadena)):\n",
        "        cadena[m] = [0]\n",
        "    elim = 0\n",
        "    posiciones = []\n",
        "    for secuencia, num_sec in zip(texto, range(0,cadena.shape[0])):\n",
        "        validador = len(secuencia)\n",
        "        idx = 1\n",
        "        if len(secuencia) < 5:\n",
        "            elim += 1\n",
        "            posiciones.append([num_sec])\n",
        "            continue\n",
        "        for elem in secuencia:\n",
        "            cadena[num_sec] = np.append(cadena[num_sec],  elem)\n",
        "            if idx < validador:\n",
        "                cadena[num_sec] = np.append(cadena[num_sec],  [-1])\n",
        "            idx += 1\n",
        "    cadena = cadena[:len(cadena)-elim]\n",
        "    return cadena, posiciones\n",
        "if __name__ == '__main__':\n",
        "    pckl = pd.read_pickle(\"conjunto_total.pckl\")\n",
        "\n",
        "    tag_to_indx = {\"Agua\": 1, \"Alto\": 2, \"Apagar\": 3, \"Ayudar\": 4, \"Campo\": 5, \"Crecer\": 6, \"Cuerpo\": 7, \"Despedir\": 8,\n",
        "                   \"Dinero\": 9, \"Escribir\": 10, \"Foto\": 11, \"Fruto\": 12, \"Hacer\": 13, \"LavarCara\": 14, \"Madera\": 15,\n",
        "                   \"Miedo\": 16, \"Morder\": 17, \"Nariz\": 18, \"Padrino\": 19, \"Película\": 20, \"Persona\": 21, \"Primero\": 22,\n",
        "                   \"Quedar\": 23, \"Saber\": 24, \"Sangre\": 25, \"Señorita\": 26, \"Suerte\": 27, \"Toro\": 28}\n",
        "    indx_to_tag = {1: \"Agua\", 2: \"Alto\", 3: \"Apagar\", 4: \"Ayudar\", 5: \"Campo\", 6: \"Crecer\", 7: \"Cuerpo\", 8: \"Despedir\",\n",
        "                   9: \"Dinero\", 10: \"Escribir\", 11: \"Foto\", 12: \"Fruto\", 13: \"Hacer\", 14: \"LavarCara\", 15: \"Madera\",\n",
        "                   16: \"Miedo\", 17: \"Morder\", 18: \"Nariz\", 19: \"Padrino\", 20: \"Película\", 21: \"Persona\", 22: \"Primero\",\n",
        "                   23: \"Quedar\", 24: \"Saber\", 25: \"Sangre\", 26: \"Señorita\", 27: \"Suerte\", 28: \"Toro\"}\n",
        "    X = pckl.iloc[:, 0].values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    max_h1, max_w1, max_h2, max_w2 = 0, 0, 0, 0\n",
        "    for p in X:\n",
        "        for m in p:\n",
        "            if m[2] > max_h1:\n",
        "                max_h1 = m[2]\n",
        "            if m[3] > max_w1:\n",
        "                max_w1 = m[3]\n",
        "            if m[4] > max_h2:\n",
        "                max_h2 = m[4]\n",
        "            if m[5] > max_w2:\n",
        "                max_w2 = m[5]\n",
        "\n",
        "\n",
        "    X = np.array([procesaEntrada(l, max_h1, max_w1, max_h2, max_w2) for l in X])#convierteATexto()\n",
        "    y = pckl.iloc[:, 1].values\n",
        "    \n",
        "    # Se procesa la entrada \n",
        "    y = [tag_to_indx[x]-1 for x in y]\n",
        "    y = to_categorical(y)\n",
        "    X = [np.array(l) for l in X]\n",
        "    MAX_LENGTH = max([len(i)  for i in X])\n",
        "\n",
        "\n",
        "    # Se hacen a todos los elementos de la entrada del mismo tamaño\n",
        "    arreglo_total = pad_sequences(maxlen=MAX_LENGTH, sequences=X, padding=\"post\")\n",
        "\n",
        "\n",
        "    # Se divide el conjunto total en conjuntos de entrenamiento, prueba y dev\n",
        "    X_train, X_test, y_train, y_test = train_test_split(arreglo_total, y, test_size=0.2)\n",
        "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1)\n",
        "    np.save(\"trainX.npy\", X_train)\n",
        "    np.save(\"trainY.npy\", y_train)\n",
        "    np.save(\"devX.npy\", X_dev)\n",
        "    np.save(\"devY.npy\", y_dev)\n",
        "    np.save(\"testX.npy\",  X_test)\n",
        "    np.save(\"testY.npy\", y_test)\n",
        "    \n",
        "\n",
        "    # Se define al modelo\n",
        "    model = Sequential()   \n",
        "    trainX = X_train.reshape((X_train.shape[0], X_train.shape[1] *  X_train.shape[2]))\n",
        "    devX = X_dev.reshape((X_dev.shape[0], X_dev.shape[1] *  X_dev.shape[2]))\n",
        "    tam_input = 60\n",
        "    model.add(LSTM(300, return_sequences=True, input_shape=(tam_input, trainX.shape[1])))\n",
        "    model.add(LSTM(300, input_shape=(tam_input, trainX.shape[1])))\n",
        "    model.add(Dropout(0.70))\n",
        "    #model.add(Flatten())\n",
        "    model.add(Dense(28, activation='softmax'))\n",
        "    opt = Adam(lr = 0.001, clipnorm=.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    epochs = 1000\n",
        "    batch_size = 2500\n",
        "\n",
        "\n",
        "    # Se definen generadores \n",
        "    generator = TimeseriesGenerator(trainX, y_train, length=tam_input, batch_size=batch_size)\n",
        "    generator2 = TimeseriesGenerator(devX, y_dev, length=tam_input, batch_size=batch_size)\n",
        "\n",
        "    # Se entrena al modelo\n",
        "    H = model.fit_generator(generator,validation_data=generator2,epochs=epochs, steps_per_epoch=tam_input, callbacks=[EarlyStopping(monitor='acc', patience=10, min_delta=0.0001)])\n",
        "  \n",
        "    model.save('modelo_trescientas_unidades_conjunto_propio.h5')\n",
        "    \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQUIz9mxDoKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "fd3ff499-91a1-4169-f7ae-2ef15da9dbbb"
      },
      "source": [
        "import keras as keras\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "model = load_model('modelo_trescientas_unidades_conjunto_propio (1).h5')\n",
        "print(model.summary())\n",
        "X = np.load('trainX.npy')\n",
        "Y = np.load('trainY.npy')\n",
        "testX = X.reshape((X.shape[0], X.shape[1] *  X.shape[2]))\n",
        "batch_size = 2500\n",
        "tam_input = 60\n",
        "nb_validation_samples = 100\n",
        "generator = TimeseriesGenerator(testX, Y, length=tam_input, batch_size=batch_size)                                                                \n",
        "score = model.evaluate_generator(generator, nb_validation_samples/batch_size)\n",
        "scores = model.predict_generator(generator, nb_validation_samples/batch_size)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 60, 300)           8331600   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 28)                8428      \n",
            "=================================================================\n",
            "Total params: 9,061,228\n",
            "Trainable params: 9,061,228\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[0.021495221182703972, 0.9986666440963745]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh0s1-4XD0kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd77ca40-5d7e-4043-a1be-d0f858d93a48"
      },
      "source": [
        "\n",
        "X = np.load('testX.npy')\n",
        "Y = np.load('testY.npy')\n",
        "testX = X.reshape((X.shape[0], X.shape[1] *  X.shape[2]))\n",
        "batch_size = 2500\n",
        "tam_input = 60\n",
        "nb_validation_samples = 100\n",
        "generator = TimeseriesGenerator(testX, Y, length=tam_input, batch_size=batch_size)                                                                \n",
        "score = model.evaluate_generator(generator, nb_validation_samples/batch_size)\n",
        "scores = model.predict_generator(generator, nb_validation_samples/batch_size)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.803570747375488, 0.0181818176060915]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}